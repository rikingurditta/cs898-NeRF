{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf8dc99-d12c-4f24-b363-6e46712090cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7fa91f-5a3d-4c38-a01a-c17f2acf549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device = torch.device(\"mps\")\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "dtype = torch.float\n",
    "torch.set_default_dtype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381a957b-4f27-41b4-9f92-c0c7485bf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera:\n",
    "    def __init__(self, pos, u, v, w, focal_length, size, render_distance, resolution):\n",
    "        self.pos = pos\n",
    "        # orthonormal basis for camera POV\n",
    "        self.u = u  # left\n",
    "        self.v = v  # up\n",
    "        self.w = w  # forward (viewing direction)\n",
    "        self.d = focal_length  # distance from pos to pixels\n",
    "        self.size = size\n",
    "        self.max_dist = render_distance;\n",
    "        self.res = resolution\n",
    "\n",
    "    def pixel_pos(self, i, j):\n",
    "        width, height = self.res\n",
    "        w_ratio = width / max(width, height)\n",
    "        h_ratio = height / max(width, height)\n",
    "        if i < 0 or j < 0 or i >= width or j >= height:\n",
    "            raise IndexError('Pixel out of bounds')\n",
    "        return self.pos + self.d * self.w \\\n",
    "                        + w_ratio * self.size / 2 * (i / width - 0.5) * self.u \\\n",
    "                        + h_ratio * self.size / 2 * (j / height - 0.5) * self.v\n",
    "\n",
    "    def pixel_dir(self, i, j):\n",
    "        dir = self.pixel_pos(i, j) - self.pos\n",
    "        return dir / dir.norm()\n",
    "\n",
    "    def index(self, i, j):\n",
    "        width, height = self.res\n",
    "        return i * height + j\n",
    "\n",
    "    def all_pixel_dirs(self):\n",
    "        width, height = self.res\n",
    "        dirs = torch.zeros((width * height, 3))\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                dirs[self.index(i, j), :] = self.pixel_dir(i, j)\n",
    "        return dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78930957-7faf-42e6-ac07-c70b28943958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlenderDefaultCamera(Camera):\n",
    "    def __init__(self, pos, u, v, w):\n",
    "        super().__init__(pos, u, v, w, 0.005, 0.036, 50, (25, 25))\n",
    "\n",
    "class BlenderOrbitCamera(BlenderDefaultCamera):\n",
    "    def __init__(self, pos):\n",
    "        u, v, w = camera_dir_from_pos(pos)\n",
    "        super().__init__(pos, u, v, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50307d6-7e65-4c49-9cfa-d9ce7fea23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_dir_from_pos(pos):\n",
    "    x, y, z = pos\n",
    "    x_angle = np.pi / 2 - torch.arctan2(z, torch.sqrt(x ** 2 + y ** 2))\n",
    "    z_angle = np.pi / 2 + torch.arctan2(y, x)\n",
    "    rotation = R.from_euler('xyz', [x_angle, 0, z_angle])\n",
    "    M = torch.from_numpy(rotation.as_matrix()).float()\n",
    "    return (M[:, 0], M[:, 1], M[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480c7057-b3f9-4b22-a0d8-a425fc7566ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/matplotlib/cm.py:489: RuntimeWarning: invalid value encountered in cast\n",
      "  xx = (xx * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NeRF(torch.nn.Module):\n",
    "    def __init__(self, training_dir):\n",
    "        super().__init__()\n",
    "        self.layer_size = 2\n",
    "        size = self.layer_size\n",
    "        self.d1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.d2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(size + 3, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, size + 1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.c = torch.nn.Sequential(\n",
    "            torch.nn.Linear(size + 3, size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size, 3),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.dir = training_dir\n",
    "        self.camera_positions = torch.from_numpy(np.genfromtxt(f'{self.dir}/coords.csv', delimiter=',')).float()\n",
    "\n",
    "\n",
    "    def get_camera_for_pos(i):\n",
    "        return BlenderOrbitCamera(self.camera_positions[i, :])\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input[:, :3]\n",
    "        dir = input[:, 3:]\n",
    "        curr = self.d1(x)\n",
    "        curr = torch.hstack((curr, x))\n",
    "        curr = self.d2(curr)\n",
    "        density = curr[:, -1].unsqueeze(1)\n",
    "        curr = torch.hstack((curr[:, :-1], dir))\n",
    "        curr = self.c(curr)\n",
    "        output = torch.hstack((density, curr))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def render(self, camera, samples_per_ray):\n",
    "        width, height = camera.res\n",
    "        pixels = torch.zeros((width * height, 3), requires_grad=True)\n",
    "        h = camera.max_dist / samples_per_ray\n",
    "        origins = torch.tile(camera.pos, (width * height, 1)).to(device)\n",
    "        dirs = camera.all_pixel_dirs().to(device)\n",
    "        prev_t = 0\n",
    "        density_so_far = torch.zeros((width * height,))\n",
    "        # it = 0\n",
    "        for s in torch.arange(0, camera.max_dist, h):\n",
    "            t = s + torch.rand((1,)) * h\n",
    "            x = origins + t * dirs\n",
    "            \n",
    "            input = torch.hstack((x, dirs))\n",
    "\n",
    "            # HERE -------------------------------\n",
    "            p = self(input)\n",
    "            # HERE -------------------------------\n",
    "\n",
    "            delta = (t - prev_t)\n",
    "\n",
    "            density = p[:, 0]\n",
    "            colour = p[:, 1:]\n",
    "            # print(colour)\n",
    "\n",
    "            density_so_far = density_so_far + delta * density\n",
    "            T = torch.exp(-density_so_far)\n",
    "            alpha = 1 - torch.exp(-delta * density)\n",
    "\n",
    "            pixels = pixels + (T * alpha).unsqueeze(1) * colour\n",
    "            \n",
    "            prev_t = t\n",
    "\n",
    "        return pixels.reshape((width, height, 3))\n",
    "\n",
    "\n",
    "    def view_image(self, camera, samples_per_ray=10):\n",
    "        rgb_img_torch = self.render(camera, samples_per_ray)\n",
    "        rgb_img_torch = rgb_img_torch / rgb_img_torch.max()\n",
    "        # rgb_img_torch = torch.softmax(rgb_img_torch, axis=2)\n",
    "        # print(rgb_img_torch)\n",
    "        rgb_img_cpu = rgb_img_torch.cpu().detach().numpy()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(rgb_img_cpu)\n",
    "\n",
    "model = NeRF('./cube_25x25')\n",
    "ex = torch.tensor([1, 0, 0])\n",
    "ey = torch.tensor([0, 1, 0])\n",
    "ez = torch.tensor([0, 0, 1])\n",
    "c = BlenderDefaultCamera(pos=-ez, u=ex, v=ey, w=ez)\n",
    "model.view_image(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dfd9aa-6aa1-4429-a510-657c18c0aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1.0.weight', Parameter containing:\n",
      "tensor([[-0.1302, -0.3106, -0.4755],\n",
      "        [ 0.1214, -0.0128, -0.2177]], requires_grad=True))\n",
      "('d1.0.bias', Parameter containing:\n",
      "tensor([ 0.4911, -0.3869], requires_grad=True))\n",
      "('d1.2.weight', Parameter containing:\n",
      "tensor([[ 0.1491,  0.5311],\n",
      "        [-0.1760,  0.4866]], requires_grad=True))\n",
      "('d1.2.bias', Parameter containing:\n",
      "tensor([-0.5521, -0.6144], requires_grad=True))\n",
      "('d1.4.weight', Parameter containing:\n",
      "tensor([[-0.5076,  0.1401],\n",
      "        [-0.1769,  0.4732]], requires_grad=True))\n",
      "('d1.4.bias', Parameter containing:\n",
      "tensor([ 0.5864, -0.4850], requires_grad=True))\n",
      "('d1.6.weight', Parameter containing:\n",
      "tensor([[-0.5110,  0.2456],\n",
      "        [-0.4564, -0.6928]], requires_grad=True))\n",
      "('d1.6.bias', Parameter containing:\n",
      "tensor([-0.0080, -0.6480], requires_grad=True))\n",
      "('d2.0.weight', Parameter containing:\n",
      "tensor([[-0.0786,  0.0152,  0.2953,  0.3190, -0.2315],\n",
      "        [-0.3036, -0.1394,  0.3061, -0.0703,  0.2573]], requires_grad=True))\n",
      "('d2.0.bias', Parameter containing:\n",
      "tensor([-0.3283,  0.2692], requires_grad=True))\n",
      "('d2.2.weight', Parameter containing:\n",
      "tensor([[ 0.6520,  0.1112],\n",
      "        [ 0.3979, -0.3128]], requires_grad=True))\n",
      "('d2.2.bias', Parameter containing:\n",
      "tensor([-0.7057,  0.1040], requires_grad=True))\n",
      "('d2.4.weight', Parameter containing:\n",
      "tensor([[-0.2013,  0.1527],\n",
      "        [-0.4613,  0.1006]], requires_grad=True))\n",
      "('d2.4.bias', Parameter containing:\n",
      "tensor([-0.0640, -0.7014], requires_grad=True))\n",
      "('d2.6.weight', Parameter containing:\n",
      "tensor([[-0.6109, -0.3996],\n",
      "        [ 0.6867, -0.1821],\n",
      "        [ 0.3552,  0.5978]], requires_grad=True))\n",
      "('d2.6.bias', Parameter containing:\n",
      "tensor([ 0.5220, -0.0026, -0.5487], requires_grad=True))\n",
      "('c.0.weight', Parameter containing:\n",
      "tensor([[ 0.1472,  0.1163,  0.0102, -0.1962, -0.1658],\n",
      "        [-0.0615,  0.2503, -0.3005, -0.4374, -0.4046]], requires_grad=True))\n",
      "('c.0.bias', Parameter containing:\n",
      "tensor([0.2327, 0.0893], requires_grad=True))\n",
      "('c.2.weight', Parameter containing:\n",
      "tensor([[-0.5131, -0.4632],\n",
      "        [ 0.1024,  0.3130],\n",
      "        [-0.2063, -0.2835]], requires_grad=True))\n",
      "('c.2.bias', Parameter containing:\n",
      "tensor([-0.3392,  0.1146,  0.5626], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f991199e-c775-4c63-8a81-9c9b6230e64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000],\n",
       "        [0.8000, 1.0000, 1.2000],\n",
       "        [2.1000, 2.4000, 2.7000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "d = torch.tensor([0.1, 0.2, 0.3]).unsqueeze(1)\n",
    "d * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fbd455-ef80-4c10-9290-2f6521be98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeRF('./cube_25x25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1236522-845b-4908-8feb-066d7128c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0----------------\n",
      "0\t0 1 2 3 4 5 6 7 8 9 \n",
      "\n",
      "\t12421835.0\n",
      "\t\n",
      " 0.0947289764881134  0.0037077467422932386  0.058428794145584106  0.008313433267176151  0.0571596696972847  0.018401602283120155  0.05062029883265495  0.04156184569001198  3023.81591796875  739.1262817382812  7708.25732421875  4010.244873046875  5887.45556640625  8177.333984375  17135.8515625  54123.50390625  3486.94287109375  3735.440185546875  1363.95751953125  35795.7109375 \n",
      " 0.094728983938694 0.0037077339366078377 0.058428794145584106 0.008313433267176151 0.057159654796123505 0.0184016115963459 0.050620295107364655 0.04156184196472168 3023.81591796875 739.1262817382812 7708.25732421875 4010.244873046875 5887.45556640625 8177.333984375 17135.8515625 54123.50390625 3486.94287109375 3735.440185546875 1363.95751953125 35795.7109375\n",
      "1\t10 11 12 13 14 15 16 17 18 19 \n",
      "\n",
      "\t15318040.0\n",
      "\t\n",
      " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 \n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "2\t20 21 22 23 24 25 26 27 28 29 \n",
      "\n",
      "\t12652568.0\n",
      "\t\n",
      " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 \n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "3\t30 31 32 33 34 35 36 37 38 39 \n",
      "\n",
      "\t14491241.0\n",
      "\t\n",
      " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 \n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "4\t40 41 42 43 44 45 46 47 48 49 \n",
      "\n",
      "\t15783781.0\n",
      "\t\n",
      " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 \n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "5\t50 51 52 53 54 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m cam \u001b[38;5;241m=\u001b[39m BlenderOrbitCamera(pos \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcamera_positions[i,:])\n\u001b[1;32m     17\u001b[0m gt_img \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m3\u001b[39m,:]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m gen_img \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(gt_img, gen_img)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m (gt_img \u001b[38;5;241m-\u001b[39m gen_img)\u001b[38;5;241m.\u001b[39mnorm() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 59\u001b[0m, in \u001b[0;36mNeRF.render\u001b[0;34m(self, camera, samples_per_ray)\u001b[0m\n\u001b[1;32m     57\u001b[0m h \u001b[38;5;241m=\u001b[39m camera\u001b[38;5;241m.\u001b[39mmax_dist \u001b[38;5;241m/\u001b[39m samples_per_ray\n\u001b[1;32m     58\u001b[0m origins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtile(camera\u001b[38;5;241m.\u001b[39mpos, (width \u001b[38;5;241m*\u001b[39m height, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 59\u001b[0m dirs \u001b[38;5;241m=\u001b[39m \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_pixel_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     60\u001b[0m prev_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m density_so_far \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((width \u001b[38;5;241m*\u001b[39m height,))\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mCamera.all_pixel_dirs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(width):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(height):\n\u001b[0;32m---> 36\u001b[0m         dirs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex(i, j), :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dirs\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mCamera.pixel_dir\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpixel_dir\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, j):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_pos(i, j) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:737\u001b[0m, in \u001b[0;36mTensor.norm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"See :func:`torch.norm`\"\"\"\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m, p, dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/overrides.py:1603\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# Check for __torch_function__ mode.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[0;32m-> 1603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_pop_mode_temporarily\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NeRF('./cube_25x25')\n",
    "N, _ = model.camera_positions.size()\n",
    "block_size = 10\n",
    "\n",
    "learning_rate = 1\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for k in range(5):\n",
    "    print(f'{k}----------------')\n",
    "    for j in range(N // block_size):\n",
    "        print(f'{j}\\t', end='')\n",
    "        running_loss = torch.zeros(1, requires_grad=True)\n",
    "        for i in range(j * block_size, min((j + 1) * block_size, N)):\n",
    "            print(f'{i} ', end='')\n",
    "            cam = BlenderOrbitCamera(pos = model.camera_positions[i,:])\n",
    "            gt_img = read_image(f'{model.dir}/{i}.png')[:3,:].permute(1, 2, 0).float().to(device)\n",
    "            gen_img = model.render(cam, 100)\n",
    "            \n",
    "            # loss = loss_fn(gt_img, gen_img)\n",
    "            loss = (gt_img - gen_img).norm() ** 2\n",
    "            running_loss = running_loss + loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print()\n",
    "        running_loss.backward()\n",
    "        print(f'\\n\\t{running_loss.item()}\\n\\t')\n",
    "        befores = []\n",
    "        for param in model.parameters():\n",
    "            print(f' {param.grad.norm()}', end=' ')\n",
    "            befores.append(param.data.clone())\n",
    "        print()\n",
    "        optimizer.step()\n",
    "        c = 0\n",
    "        for param in model.parameters():\n",
    "            print(f' {(param.data - befores[c]).norm()}', end='')\n",
    "            c += 1\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60eec1e-4ce6-44ce-9b83-b445da4fe6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.081462 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1\n",
    "pos = model.camera_positions[i,:]\n",
    "u, v, w = camera_dir_from_pos(pos)\n",
    "cam = BlenderDefaultCamera(pos, u, v, w)\n",
    "gt_img = read_image(f'{model.dir}/{i}.png')[:3,:].permute(1, 2, 0).float() / 255\n",
    "gen_img = model.render(cam, 1000)\n",
    "\n",
    "gt_img_cpu = gt_img.cpu().detach().numpy()\n",
    "gen_img_cpu = gen_img.cpu().detach().numpy()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax1.imshow(gt_img_cpu)\n",
    "ax2.imshow(gen_img_cpu)\n",
    "\n",
    "print(np.linalg.norm(gt_img_cpu), np.linalg.norm(gen_img_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa222428-ab2a-4fdc-a06f-b1650b2dae18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4551])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((1,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
